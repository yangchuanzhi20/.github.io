

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.jpg">
  <link rel="icon" href="/img/favicon.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="白色很哇塞">
  <meta name="keywords" content="发疯，生活">
  
    <meta name="description" content="pytorch基础知识梳理🍧">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch基础!🍧">
<meta property="og:url" content="https://yangchuanzhi20.github.io/2024/01/24/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/Pytorch/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/pytorch%E5%9F%BA%E7%A1%80/index.html">
<meta property="og:site_name" content="小杨的精神世界ヽ( ຶ▮ ຶ)ﾉ">
<meta property="og:description" content="pytorch基础知识梳理🍧">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://yangchuanzhi20.github.io/img/39.jpg">
<meta property="article:published_time" content="2024-01-24T06:34:19.000Z">
<meta property="article:modified_time" content="2024-02-14T10:33:16.809Z">
<meta property="article:author" content="白色很哇塞">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="Pytorch">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://yangchuanzhi20.github.io/img/39.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>pytorch基础!🍧 ˙ϖ˙ 小杨的精神世界ヽ( ຶ▮ ຶ)ﾉ</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"yangchuanzhi20.github.io","root":"/","version":"1.9.5-a","typing":{"enable":true,"typeSpeed":70,"cursorChar":"♡","loop":false,"scope":["home"]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body><!-- hexo injector body_begin start --><div id="web_bg"></div><!-- hexo injector body_begin end -->
  

  <header>
    

<div class="header-inner" style="height: 100vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>白色很哇塞ฅ՞•ﻌ•՞ฅ</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/game/">
                <i class="iconfont icon-switch-fill"></i>
                <span>游戏</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/39.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle">pytorch基础!🍧</span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-01-24 14:34" pubdate>
          2024年1月24日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          17k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          138 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
        <div class="scroll-down-bar">
          <i class="iconfont icon-arrowdown"></i>
        </div>
      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">pytorch基础!🍧</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="Pytorch基础"><a href="#Pytorch基础" class="headerlink" title="Pytorch基础"></a>Pytorch基础</h1><h2 id="1-Tensors张量"><a href="#1-Tensors张量" class="headerlink" title="1.Tensors张量"></a>1.Tensors张量</h2><p>张量是一种专门的数据结构，与数组和矩阵非常相似。在 PyTorch 中，我们使用张量对模型的输入和输出以及模型的参数进行编码。</p>
<p>张量类似于 NumPy 的 ndarrays，不同之处在于张量可以在 GPU 或其他硬件加速器上运行。事实上，张量和 NumPy 数组通常可以共享相同的底层内存，从而消除了复制数据的需要。</p>
<h3 id="1-1-初始化张量"><a href="#1-1-初始化张量" class="headerlink" title="1.1 初始化张量"></a>1.1 初始化张量</h3><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br></code></pre></td></tr></table></figure>

<p>张量可以通过多种方式进行初始化。</p>
<blockquote>
<p><strong>直接来自数据</strong></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">data = [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]<br>x_data = torch.tensor(data)<br></code></pre></td></tr></table></figure>

<blockquote>
<p><strong>从 NumPy 数组</strong></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">np_array = np.array(data)<br>x_np = torch.from_numpy(np_array)<br></code></pre></td></tr></table></figure>

<blockquote>
<p><strong>从另一个张量</strong></p>
</blockquote>
<p>新张量保留参数张量的属性（形状、数据类型），除非显式覆盖。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">x_ones = torch.ones_like(x_data) <span class="hljs-comment"># retains the properties of x_data</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Ones Tensor: \n <span class="hljs-subst">&#123;x_ones&#125;</span> \n&quot;</span>)<br><br>x_rand = torch.rand_like(x_data, dtype=torch.<span class="hljs-built_in">float</span>) <span class="hljs-comment"># overrides the datatype of x_data</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Random Tensor: \n <span class="hljs-subst">&#123;x_rand&#125;</span> \n&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">Ones Tensor:<br> tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>],<br>        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]])<br><br>Random Tensor:<br> tensor([[<span class="hljs-number">0.8823</span>, <span class="hljs-number">0.9150</span>],<br>        [<span class="hljs-number">0.3829</span>, <span class="hljs-number">0.9593</span>]])<br></code></pre></td></tr></table></figure>

<blockquote>
<p><strong>使用随机值或常量值</strong></p>
</blockquote>
<p><code>shape</code> 是张量维度的元组。在下面的函数中，它决定了输出张量的维数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">shape = (<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,)<br>rand_tensor = torch.rand(shape)<br>ones_tensor = torch.ones(shape)<br>zeros_tensor = torch.zeros(shape)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Random Tensor: \n <span class="hljs-subst">&#123;rand_tensor&#125;</span> \n&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Ones Tensor: \n <span class="hljs-subst">&#123;ones_tensor&#125;</span> \n&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Zeros Tensor: \n <span class="hljs-subst">&#123;zeros_tensor&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>

<p><strong>tips：</strong>在给定形状（shape）时，逗号在 Python 中通常用于表示元组。在这种情况下，<code>shape = (2, 3,)</code> 中的逗号实际上是一个元组的标志，即使在没有逗号的情况下，它也是一个合法的元组。</p>
<p>这种写法是为了确保在定义元组时即使只有一个元素也使用逗号，以<strong>避免与普通的括号运算符产生歧义</strong>。例如，如果写成 <code>shape = (2, 3)</code>，它将被解释为一个包含两个整数的表达式，而不是一个包含一个元组的表达式。</p>
<p>在 Python 中，单个元素的元组需要在元素后面添加逗号，以明确表示这是一个元组。这是为了区分元组和括号内的表达式。所以，<code>(2, 3,)</code> 和 <code>(2, 3)</code> <strong>在这里是等价</strong>的，都表示一个包含两个整数的元组。</p>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">Random Tensor:<br> tensor([[<span class="hljs-number">0.3904</span>, <span class="hljs-number">0.6009</span>, <span class="hljs-number">0.2566</span>],<br>        [<span class="hljs-number">0.7936</span>, <span class="hljs-number">0.9408</span>, <span class="hljs-number">0.1332</span>]])<br><br>Ones Tensor:<br> tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]])<br><br>Zeros Tensor:<br> tensor([[<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],<br>        [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>]])<br></code></pre></td></tr></table></figure>

<h3 id="1-2-张量的属性"><a href="#1-2-张量的属性" class="headerlink" title="1.2 张量的属性"></a>1.2 张量的属性</h3><p>张量属性描述它们的形状、数据类型和存储它们的设备。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor = torch.rand(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Shape of tensor: <span class="hljs-subst">&#123;tensor.shape&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Datatype of tensor: <span class="hljs-subst">&#123;tensor.dtype&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Device tensor is stored on: <span class="hljs-subst">&#123;tensor.device&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">Shape of tensor: torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">4</span>])<br>Datatype of tensor: torch.float32<br>Device tensor <span class="hljs-keyword">is</span> stored on: cpu<br></code></pre></td></tr></table></figure>

<h3 id="1-3-张量操作"><a href="#1-3-张量操作" class="headerlink" title="1.3 张量操作"></a>1.3 张量操作</h3><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html">https://pytorch.org/docs/stable/torch.html</a></p>
<p>这里全面描述了 100 多种张量运算，包括算术、线性代数、矩阵操作（转置、索引、切片）、采样等。这些操作中的每一个都可以在 GPU 上运行（速度通常高于在 CPU 上）。</p>
<p>默认情况下，张量是在 CPU 上创建的。我们需要显式地将张量移动到 GPU using <code>.to</code> 方法（在检查 GPU 可用性之后）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># We move our tensor to the GPU if available</span><br><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>    tensor = tensor.to(<span class="hljs-string">&quot;cuda&quot;</span>)<br></code></pre></td></tr></table></figure>

<blockquote>
<p><strong>索引和切片</strong></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor = torch.ones(<span class="hljs-number">4</span>, <span class="hljs-number">4</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;First row: <span class="hljs-subst">&#123;tensor[<span class="hljs-number">0</span>]&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;First column: <span class="hljs-subst">&#123;tensor[:, <span class="hljs-number">0</span>]&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Last column: <span class="hljs-subst">&#123;tensor[..., -<span class="hljs-number">1</span>]&#125;</span>&quot;</span>)<br>tensor[:,<span class="hljs-number">1</span>] = <span class="hljs-number">0</span><br><span class="hljs-built_in">print</span>(tensor)<br></code></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">First row: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>])<br>First column: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>])<br>Last column: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>])<br>tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]])<br></code></pre></td></tr></table></figure>

<p><strong>tips：</strong>省略符号的作用是<strong>省略掉其余的维度</strong>。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor = torch.rand((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>,<span class="hljs-number">4</span>))<br><span class="hljs-built_in">print</span>(tensor)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;tensor[:,:,:,<span class="hljs-number">1</span>]&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;tensor[..., <span class="hljs-number">1</span>]&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>

<p><code>...</code>等价于<code>:,:,:</code></p>
<p>连接张量 可用于 <code>torch.cat</code> 沿给定维度连接一系列张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t1 = torch.cat([tensor, tensor, tensor], dim=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(t1)<br></code></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]])<br></code></pre></td></tr></table></figure>

<p><strong>tips：</strong>dim表示连接的维度。</p>
<blockquote>
<p><strong>算术运算</strong></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value</span><br><span class="hljs-comment"># ``tensor.T`` returns the transpose of a tensor 转置矩阵</span><br>y1 = tensor @ tensor.T<br>y2 = tensor.matmul(tensor.T)<br><br>y3 = torch.rand_like(y1)<br>torch.matmul(tensor, tensor.T, out=y3)<br><br><br><span class="hljs-comment"># This computes the element-wise product. z1, z2, z3 will have the same value</span><br>z1 = tensor * tensor<br>z2 = tensor.mul(tensor)<br><br>z3 = torch.rand_like(tensor)<br>torch.mul(tensor, tensor, out=z3)<br></code></pre></td></tr></table></figure>

<ol>
<li><strong>矩阵乘法（Matrix Multiplication）</strong>：<ul>
<li><code>y1 = tensor @ tensor.T</code>：这是Python中使用<code>@</code>运算符进行<strong>矩阵乘法</strong>的简洁写法，它计算了<code>tensor</code>与其转置的矩阵相乘，并将结果存储在<code>y1</code>中。</li>
<li><code>y2 = tensor.matmul(tensor.T)</code>：这是<code>torch.Tensor</code>类的<code>matmul</code>方法的调用方式，实现了与<code>@</code>运算符相同的功能，将两个张量相乘。</li>
<li><code>y3 = torch.rand_like(y1)</code>和<code>torch.matmul(tensor, tensor.T, out=y3)</code>：这两行代码将矩阵乘法的结果存储在预先分配的张量<code>y3</code>中。</li>
</ul>
</li>
<li><strong>元素级乘法（Element-wise Multiplication）</strong>：<ul>
<li><code>z1 = tensor * tensor</code>：这是Python中进行元素级乘法的简洁写法，它将<code>tensor</code>中的每个元素与另一个<code>tensor</code>中<strong>对应位置的元素相乘</strong>，并将结果存储在<code>z1</code>中。</li>
<li><code>z2 = tensor.mul(tensor)</code>：这是<code>torch.Tensor</code>类的<code>mul</code>方法的调用方式，实现了与<code>*</code>运算符相同的功能，进行元素级乘法。</li>
<li><code>z3 = torch.rand_like(tensor)</code>和<code>torch.mul(tensor, tensor, out=z3)</code>：这两行代码将元素级乘法的结果存储在预先分配的张量<code>z3</code>中。</li>
</ul>
</li>
</ol>
<blockquote>
<p><strong>单元素张量</strong></p>
</blockquote>
<p>如果你有一个单元素张量，例如通过将张量的所有值聚合为一个值，你可以使用以下命令 <code>item()</code> 将其转换为 Python 数值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">agg = tensor.<span class="hljs-built_in">sum</span>()<br>agg_item = agg.item()<br><span class="hljs-built_in">print</span>(agg_item, <span class="hljs-built_in">type</span>(agg_item))<br></code></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-number">12.0</span> &lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;float&#x27;</span>&gt;<br></code></pre></td></tr></table></figure>

<p><strong>tips：</strong>agg也是一个张量，为单元素张量。</p>
<blockquote>
<p><strong>就地操作</strong></p>
</blockquote>
<p>将结果存储到操作数中的操作称为就地操作。它们由 <code>_</code> 后缀表示。例如： <code>x.copy_(y)</code> 、、 <code>x.t_()</code> 将更改 <code>x</code> 。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;tensor&#125;</span> \n&quot;</span>)<br>tensor.add_(<span class="hljs-number">5</span>)<br><span class="hljs-built_in">print</span>(tensor)<br></code></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]])<br><br>tensor([[<span class="hljs-number">6.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">6.</span>],<br>        [<span class="hljs-number">6.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">6.</span>],<br>        [<span class="hljs-number">6.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">6.</span>],<br>        [<span class="hljs-number">6.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">6.</span>]])<br></code></pre></td></tr></table></figure>

<p>常见的PyTorch就地操作：</p>
<ol>
<li>**add_、sub_、mul_、div_**：<ul>
<li><code>add_()</code>：就地执行张量的加法。</li>
<li><code>sub_()</code>：就地执行张量的减法。</li>
<li><code>mul_()</code>：就地执行张量的乘法。</li>
<li><code>div_()</code>：就地执行张量的除法。</li>
</ul>
</li>
<li><strong>其他数学函数</strong>：<ul>
<li><code>abs_()</code>：就地执行张量的绝对值操作。</li>
<li><code>neg_()</code>：就地执行张量的取负操作。</li>
<li><code>pow_()</code>：就地执行张量的指数操作。</li>
<li><code>clamp_()</code>：就地执行张量的截断操作。</li>
</ul>
</li>
<li><strong>归约操作</strong>：<ul>
<li><code>sum_()</code>：就地计算张量的元素之和。</li>
<li><code>mean_()</code>：就地计算张量的平均值。</li>
<li><code>max_()</code>：就地计算张量的最大值。</li>
<li><code>min_()</code>：就地计算张量的最小值。</li>
</ul>
</li>
<li><strong>其他操作</strong>：<ul>
<li><code>fill_()</code>：用指定的标量值填充张量。</li>
<li><code>zero_()</code>：将张量的所有元素设置为0。</li>
<li><code>fill_diagonal_()</code>：将张量的对角线元素填充为指定值。</li>
</ul>
</li>
</ol>
<p>这些就地操作都是在函数名后面添加下划线<code>_</code>来表示的，例如<code>add_()</code>、<code>mul_()</code>等。在使用时需要小心，因为它们会直接修改原始的张量，可能会导致不可预测的结果或难以调试的错误。</p>
<h3 id="1-4-使用-NumPy-桥接"><a href="#1-4-使用-NumPy-桥接" class="headerlink" title="1.4 使用 NumPy 桥接"></a>1.4 使用 NumPy 桥接</h3><p>CPU 上的张量和 NumPy 数组可以共享其底层内存位置，更改一个将更改另一个。</p>
<blockquote>
<h3 id="Tensor-到-NumPy-数组"><a href="#Tensor-到-NumPy-数组" class="headerlink" title="Tensor 到 NumPy 数组"></a>Tensor 到 NumPy 数组</h3></blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">t = torch.ones(<span class="hljs-number">5</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;t: <span class="hljs-subst">&#123;t&#125;</span>&quot;</span>)<br>n = t.numpy()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;n: <span class="hljs-subst">&#123;n&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>])<br>n: [<span class="hljs-number">1.</span> <span class="hljs-number">1.</span> <span class="hljs-number">1.</span> <span class="hljs-number">1.</span> <span class="hljs-number">1.</span>]<br></code></pre></td></tr></table></figure>

<p>张量的变化反映在 NumPy 数组中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">t.add_(<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;t: <span class="hljs-subst">&#123;t&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;n: <span class="hljs-subst">&#123;n&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t: tensor([<span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>])<br>n: [<span class="hljs-number">2.</span> <span class="hljs-number">2.</span> <span class="hljs-number">2.</span> <span class="hljs-number">2.</span> <span class="hljs-number">2.</span>]<br></code></pre></td></tr></table></figure>

<blockquote>
<h3 id="NumPy-数组转-Tensor"><a href="#NumPy-数组转-Tensor" class="headerlink" title="NumPy 数组转 Tensor"></a>NumPy 数组转 Tensor</h3></blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">n = np.ones(<span class="hljs-number">5</span>)<br>t = torch.from_numpy(n)<br></code></pre></td></tr></table></figure>

<p>NumPy 数组中的更改会反映在张量中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">np.add(n, <span class="hljs-number">1</span>, out=n)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;t: <span class="hljs-subst">&#123;t&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;n: <span class="hljs-subst">&#123;n&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t: tensor([<span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>], dtype=torch.float64)<br>n: [<span class="hljs-number">2.</span> <span class="hljs-number">2.</span> <span class="hljs-number">2.</span> <span class="hljs-number">2.</span> <span class="hljs-number">2.</span>]<br></code></pre></td></tr></table></figure>

<h2 id="2-Datasets-DataLoaders-数据集和数据加载器"><a href="#2-Datasets-DataLoaders-数据集和数据加载器" class="headerlink" title="2.Datasets &amp; DataLoaders 数据集和数据加载器"></a>2.<strong>Datasets &amp; DataLoaders</strong> 数据集和数据加载器</h2><p>用于处理数据样本的代码可能会变得混乱且难以维护;理想情况下，我们希望我们的<strong>数据集代码与模型训练代码解耦</strong>，以获得更好的可读性和模块化。PyTorch 提供了两个数据原语： <code>torch.utils.data.DataLoader</code>和<code>torch.utils.data.Dataset</code> 允许你使用预加载的数据集以及你自己的数据。</p>
<p>Dataset 存储样本及其相应的标签，而 DataLoader 则在 Dataset 周围封装了一个可迭代器，以方便访问样本。</p>
<h3 id="2-1-加载数据集"><a href="#2-1-加载数据集" class="headerlink" title="2.1 加载数据集"></a>2.1 加载数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> ToTensor<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><br>training_data = datasets.FashionMNIST(<br>    root=<span class="hljs-string">&quot;data&quot;</span>,<br>    train=<span class="hljs-literal">True</span>,<br>    download=<span class="hljs-literal">True</span>,<br>    transform=ToTensor()<br>)<br><br>test_data = datasets.FashionMNIST(<br>    root=<span class="hljs-string">&quot;data&quot;</span>,<br>    train=<span class="hljs-literal">False</span>,<br>    download=<span class="hljs-literal">True</span>,<br>    transform=ToTensor()<br>)<br></code></pre></td></tr></table></figure>

<ul>
<li><code>root</code> 是存储训练&#x2F;测试数据的路径，</li>
<li><code>train</code> 指定训练或测试数据集，</li>
<li><code>download=True</code> 如果数据在 上不可用 <code>root</code> ，则从 Internet 下载数据。</li>
<li><code>transform</code> 和 <code>target_transform</code> 指定要素和标注转换</li>
</ul>
<h3 id="2-2-数据加载器"><a href="#2-2-数据加载器" class="headerlink" title="2.2 数据加载器"></a>2.2 数据加载器</h3><p>检索 <code>Dataset</code> 数据集的特征，并一次标记一个样本。在训练模型时，我们通常希望以“小批量”的方式传递样本，在每个时期重新洗牌数据以减少模型过拟合，并使用 Python <code>multiprocessing</code> 来加快数据检索速度。</p>
<p><code>DataLoader</code> 是一个可迭代的对象，它通过一个简单的 API 为我们抽象了这种复杂性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><br>train_dataloader = DataLoader(training_data, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>)<br>test_dataloader = DataLoader(test_data, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure>

<p>我们已将该数据集加载到<code>DataLoader</code>中，并可以根据需要遍历该数据集。下面的每次迭代都会返回一批 <code>train_features</code> and <code>train_labels</code> （分别包含 <code>batch_size=64</code> 特征和标签）。因为我们指定 <code>shuffle=True</code> 了 ，在我们遍历所有批次后，数据会被洗牌。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Display image and label.</span><br>train_features, train_labels = <span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(train_dataloader))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Feature batch shape: <span class="hljs-subst">&#123;train_features.size()&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Labels batch shape: <span class="hljs-subst">&#123;train_labels.size()&#125;</span>&quot;</span>)<br>img = train_features[<span class="hljs-number">0</span>].squeeze()<br>label = train_labels[<span class="hljs-number">0</span>]<br>plt.imshow(img, cmap=<span class="hljs-string">&quot;gray&quot;</span>)<br>plt.show()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Label: <span class="hljs-subst">&#123;label&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>

<p><strong>tips：</strong>tensor.shape和tensor.size()获取到的内容相同，但一个是属性，一个是方法。可以把tensor看成是一个类。</p>
<h2 id="3-构建神经网络"><a href="#3-构建神经网络" class="headerlink" title="3.构建神经网络"></a>3.构建神经网络</h2><p>神经网络由对数据执行操作的层&#x2F;模块组成。torch.nn 命名空间提供了构建自己的神经网络所需的所有构建块。PyTorch 中的每个模块都对 nn.模块。神经网络本身是由其他模块（层）组成的模块。这种嵌套结构允许轻松构建和管理复杂的架构。</p>
<h3 id="3-1-获取用于训练的设备"><a href="#3-1-获取用于训练的设备" class="headerlink" title="3.1 获取用于训练的设备"></a>3.1 获取用于训练的设备</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">device = (<br>    <span class="hljs-string">&quot;cuda&quot;</span><br>    <span class="hljs-keyword">if</span> torch.cuda.is_available()<br>    <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;mps&quot;</span><br>    <span class="hljs-keyword">if</span> torch.backends.mps.is_available()<br>    <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span><br>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Using <span class="hljs-subst">&#123;device&#125;</span> device&quot;</span>)<br></code></pre></td></tr></table></figure>

<h3 id="3-2-定义类"><a href="#3-2-定义类" class="headerlink" title="3.2 定义类"></a>3.2 定义类</h3><p>我们通过子类化来定义我们的神经网络 <code>nn.Module</code> ，并在 <code>__init__</code> 中初始化神经网络层。每个 <code>nn.Module</code> 子类都实现对方法中输入数据的 <code>forward</code> 操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">NeuralNetwork</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.flatten = nn.Flatten()<br>        self.linear_relu_stack = nn.Sequential(<br>            nn.Linear(<span class="hljs-number">28</span>*<span class="hljs-number">28</span>, <span class="hljs-number">512</span>),<br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>),<br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">10</span>),<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.flatten(x)<br>        logits = self.linear_relu_stack(x)<br>        <span class="hljs-keyword">return</span> logits<br></code></pre></td></tr></table></figure>

<p>我们创建一个 <code>NeuralNetwork</code> 的实例，并将其移动到 <code>device</code> 中，并打印其结构。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">model = NeuralNetwork().to(device)<br><span class="hljs-built_in">print</span>(model)<br></code></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">NeuralNetwork(<br>  (flatten): Flatten(start_dim=<span class="hljs-number">1</span>, end_dim=-<span class="hljs-number">1</span>)<br>  (linear_relu_stack): Sequential(<br>    (<span class="hljs-number">0</span>): Linear(in_features=<span class="hljs-number">784</span>, out_features=<span class="hljs-number">512</span>, bias=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">1</span>): ReLU()<br>    (<span class="hljs-number">2</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">512</span>, bias=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">3</span>): ReLU()<br>    (<span class="hljs-number">4</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">10</span>, bias=<span class="hljs-literal">True</span>)<br>  )<br>)<br></code></pre></td></tr></table></figure>

<p>为了使用模型，我们将输入数据传递给它。这将执行模型的 <code>forward</code> ，以及一些后台操作。不要直接调用<code>model.forward()</code> ！</p>
<p><strong>tips：</strong></p>
<ul>
<li>在 PyTorch 中，实现了 <code>nn.Module</code> 的子类中的 <code>forward</code> 方法是一个特殊的约定。当您调用模型的实例（例如 <code>model</code>）时，PyTorch 会自动调用 <code>forward</code> 方法，而不需要显式地调用 <code>model.forward()</code>。</li>
<li>这是因为<code>torch.nn.Module</code>类中已经定义了<code>__call__</code>方法，而该方法内部实际上会调用<code>forward()</code>方法。</li>
<li>在Python中，<code>__call__</code>是一个特殊方法，允许类的实例像函数一样被调用。</li>
</ul>
<p>下面是一个简单的例子，演示了如何使用<code>__call__</code>方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Multiplier</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, factor</span>):<br>        self.factor = factor<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> self.factor * x<br><br><span class="hljs-comment"># 创建一个Multiplier实例</span><br>double = Multiplier(<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># 使用__call__方法调用实例，实际上就像调用一个函数一样</span><br>result = double(<span class="hljs-number">5</span>)  <span class="hljs-comment"># 相当于调用了double.__call__(5)</span><br><span class="hljs-built_in">print</span>(result)  <span class="hljs-comment"># 输出 10</span><br></code></pre></td></tr></table></figure>

<p>在输入上调用模型将返回一个二维张量，其中 dim&#x3D;0 对应于每个类的 10 个原始预测值的每个输出，dim&#x3D;1 对应于每个输出的单个值。我们通过传递模块的 <code>nn.Softmax</code> 实例来获取预测概率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.rand(<span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, device=device)<br>logits = model(X)<br>pred_probab = nn.Softmax(dim=<span class="hljs-number">1</span>)(logits)<br>y_pred = pred_probab.argmax(<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Predicted class: <span class="hljs-subst">&#123;y_pred&#125;</span>&quot;</span>)[]()<br></code></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">Predicted <span class="hljs-keyword">class</span>: tensor([<span class="hljs-number">7</span>], device=<span class="hljs-string">&#x27;cuda:0&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p><strong>tips：</strong></p>
<ol>
<li><code>tensor.argmax(1)</code> 是对 PyTorch 张量进行操作，用于沿指定维度找到张量中最大值的索引。参数是0表示沿着列找最大值，1表示沿着行找最大值。</li>
<li><code>nn.Softmax(dim=1)</code> 是PyTorch等深度学习框架中常用的函数。该函数用于沿着指定的维度计算张量的 softmax 激活。在这里，<code>dim=1</code> 表示 softmax 沿着输入张量的第二个维度（从 0 开始索引）进行操作。</li>
</ol>
<h3 id="3-2-模型层"><a href="#3-2-模型层" class="headerlink" title="3.2 模型层"></a>3.2 模型层</h3><h4 id="nn-Flatten"><a href="#nn-Flatten" class="headerlink" title="nn.Flatten"></a>nn.Flatten</h4><p><code>nn.Flatten</code> 是 PyTorch 中的一个层（Layer），用于将输入的多维张量（例如，具有多个轴或维度的张量）转换为一个具有单个轴的张量（通常是一维张量）。其作用是将输入的数据“展平”成一个一维向量，以便于后续的神经网络层（如全连接层）处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br>input_tensor = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>,<span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(input_tensor)<br><span class="hljs-comment"># 定义一个 Flatten 层</span><br>flatten_layer = nn.Flatten()<br><br><span class="hljs-comment"># 使用 Flatten 层将输入张量展平</span><br>output = flatten_layer(input_tensor)<br><span class="hljs-built_in">print</span>(output)<br></code></pre></td></tr></table></figure>

<p>结果：</p>
<p><img src="/2024/01/24/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/Pytorch/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/pytorch%E5%9F%BA%E7%A1%80/1.png" srcset="/img/loading.gif" lazyload></p>
<p>在许多情况下，当将卷积层的输出传递给全连接层时，需要使用 <code>nn.Flatten</code> 来将卷积层的输出展平为一维张量，以便于后续的全连接层处理。</p>
<h4 id="nn-Linear"><a href="#nn-Linear" class="headerlink" title="nn.Linear"></a>nn.Linear</h4><p><code>nn.Linear</code> 是 PyTorch 中的一个线性层（Linear Layer），也称为全连接层（Fully Connected Layer）或仿射层（Affine Layer）。这个层<strong>将输入张量与权重矩阵相乘，然后加上偏置向量</strong>（可选），最后应用激活函数（也可选）。</p>
<p>在神经网络中，全连接层通常用于将输入数据与权重相乘，并加上偏置，从而产生新的特征表示，这些特征表示被传递给下一层。全连接层的作用是将输入数据映射到输出空间中。</p>
<p>以下是 <code>nn.Linear</code> 的基本用法示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">linear_layer = nn.Linear(<span class="hljs-number">8</span>, <span class="hljs-number">4</span>)<br>output = linear_layer(output)<br><span class="hljs-built_in">print</span>(output)<br></code></pre></td></tr></table></figure>

<p>结果：</p>
<p><img src="/2024/01/24/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/Pytorch/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/pytorch%E5%9F%BA%E7%A1%80/2.png" srcset="/img/loading.gif" lazyload></p>
<p>在训练神经网络时，权重矩阵和偏置向量是可学习的参数，它们会根据反向传播算法进行优化，以最小化损失函数。</p>
<p><strong>tips：</strong></p>
<p>权重矩阵和偏置向量是随机的。</p>
<p>nn.ReLU</p>
<p>非线性激活是在模型的输入和输出之间创建复杂映射的原因。它们在线性变换后应用以引入非线性，帮助神经网络学习各种现象。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">output = nn.ReLU()(output)<br><span class="hljs-built_in">print</span>(output)<br></code></pre></td></tr></table></figure>

<p>结果：</p>
<p><img src="/2024/01/24/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/Pytorch/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/pytorch%E5%9F%BA%E7%A1%80/3.png" srcset="/img/loading.gif" lazyload></p>
<p><strong>tips：</strong></p>
<ul>
<li>ReLU（Rectified Linear Unit）是一种常用的非线性激活函数，被广泛应用于深度神经网络中。ReLU函数定义为：<code>f(x)=max(0,x)</code>，即，当输入 <em>x</em> 大于等于0时，ReLU函数返回输入 <em>x</em>；当输入 <em>x</em> 小于0时，ReLU函数返回0。如上图结果所示。</li>
<li>常见的非线性激活函数包括：</li>
</ul>
<ol>
<li>Sigmoid函数：将输入映射到0到1之间的连续范围，常用于输出层的二分类问题。</li>
<li>Tanh函数：类似于Sigmoid函数，但将输入映射到-1到1之间的连续范围，也常用于隐藏层。</li>
<li>ReLU（Rectified Linear Unit）函数：对于正数输入，输出等于输入；对于负数输入，输出为0。ReLU函数在深度学习中得到了广泛应用，因为它的计算简单且在训练过程中可以加速收敛。</li>
<li>Leaky ReLU函数：与ReLU类似，但对负数输入有小的线性斜率，可以避免ReLU中的“死亡神经元”问题。</li>
<li>Softmax函数：常用于多分类问题的输出层，将输入转换成一个概率分布，使得输出的所有值都在0到1之间且总和为1。</li>
</ol>
<h4 id="nn-Sequential"><a href="#nn-Sequential" class="headerlink" title="nn.Sequential"></a>nn.Sequential</h4><p>nn.Sequential 是模块的有序容器。数据以与定义的相同的顺序传递到所有模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">linear_relu_stack = nn.Sequential(<br>            nn.Flatten(),<br>            nn.Linear(<span class="hljs-number">8</span>, <span class="hljs-number">4</span>),<br>            nn.ReLU(),<br>        )<br>output2 = linear_relu_stack(input_tensor)<br></code></pre></td></tr></table></figure>

<p>效果与之前的定义相同。</p>
<h4 id="nn-Softmax"><a href="#nn-Softmax" class="headerlink" title="nn.Softmax"></a>nn.Softmax</h4><p>Softmax函数是一种常用的激活函数，通常用于多分类问题的输出层，将原始的网络输出转换为表示概率分布的形式。Softmax函数将输入向量 <em>z</em> 的每个元素转换为一个介于0和1之间的实数，同时确保所有元素的总和为1，因此可以看作是对输入向量的归一化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">output = nn.Softmax(dim=<span class="hljs-number">1</span>)(output)<br><span class="hljs-built_in">print</span>(output)<br></code></pre></td></tr></table></figure>

<p>结果：</p>
<p><img src="/2024/01/24/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/Pytorch/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/pytorch%E5%9F%BA%E7%A1%80/4.png" srcset="/img/loading.gif" lazyload></p>
<p><strong>tips：</strong></p>
<p>dim为1表示行的和为1，dim为0表示列的和为 1。</p>
<h3 id="3-3-模型参数"><a href="#3-3-模型参数" class="headerlink" title="3.3 模型参数"></a>3.3 模型参数</h3><p>神经网络中的许多层都是参数化的，即具有相关的权重和偏差，这些权重和偏差在训练过程中得到优化。子类 <code>nn.Module</code> 会自动跟踪模型对象中定义的所有字段，并使所有参数都可以使用模型 <code>parameters()</code> 或 <code>named_parameters()</code> 方法访问。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">input</span> = torch.tensor([[<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>],<br>                      [<span class="hljs-number">4.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">6.0</span>]])  <span class="hljs-comment"># 2个样本，3个特征</span><br><br><span class="hljs-comment"># 定义权重矩阵和偏置向量</span><br>weight = torch.tensor([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>],   <span class="hljs-comment"># 2个输出特征，每个特征对应3个输入特征</span><br>                       [<span class="hljs-number">0.4</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.6</span>]])<br>bias = torch.tensor([<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>])  <span class="hljs-comment"># 2个输出特征，每个特征都有一个偏置</span><br><br><span class="hljs-comment"># 执行线性变换操作</span><br>output = F.linear(<span class="hljs-built_in">input</span>, weight, bias)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output after linear transformation:&quot;</span>)<br><span class="hljs-built_in">print</span>(output)<br></code></pre></td></tr></table></figure>

<p>结果：</p>
<p><img src="/2024/01/24/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/Pytorch/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/pytorch%E5%9F%BA%E7%A1%80/5.png" srcset="/img/loading.gif" lazyload></p>
<p><strong>tips：</strong></p>
<p>偏差会加在<strong>每一行</strong>上。</p>
<h2 id="4-torch-autograd自动微分"><a href="#4-torch-autograd自动微分" class="headerlink" title="4.torch.autograd自动微分"></a>4.torch.autograd自动微分</h2><p>在训练神经网络时，最常用的算法是反向传播。在该算法中，参数（模型权重）根据损失函数相对于给定参数的梯度进行调整。</p>
<p>为了计算这些梯度，PyTorch 有一个内置的微分引擎，称为 <code>torch.autograd</code> 。它支持自动计算任何计算图的梯度。</p>
<p>考虑最简单的单层神经网络，具有输入 <code>x</code> 、参数 <code>w</code> ， <code>b</code> 和一些损失函数。可以在 PyTorch 中按以下方式定义它：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br>x = torch.ones(<span class="hljs-number">5</span>)  <span class="hljs-comment"># input tensor</span><br>y = torch.zeros(<span class="hljs-number">3</span>)  <span class="hljs-comment"># expected output</span><br>w = torch.randn(<span class="hljs-number">5</span>, <span class="hljs-number">3</span>, requires_grad=<span class="hljs-literal">True</span>)<br>b = torch.randn(<span class="hljs-number">3</span>, requires_grad=<span class="hljs-literal">True</span>)<br>z = torch.matmul(x, w)+b<br>loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)<br></code></pre></td></tr></table></figure>

<p>PyTorch中实现自动计算梯度的机制是通过<strong>动态计算图</strong>实现的。当你在PyTorch中定义张量并进行操作时，PyTorch会构建一个计算图，该计算图描述了数据流经过的操作，并且知道每个操作涉及的张量之间的依赖关系。这个计算图是动态的，因为它在每次执行时都会重新构建。</p>
<p>此代码定义以下计算图：</p>
<p><img src="/2024/01/24/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/Pytorch/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/pytorch%E5%9F%BA%E7%A1%80/6.png" srcset="/img/loading.gif" lazyload></p>
<p>为了优化神经网络中参数的权重，我们需要计算损失函数相对于参数的导数。为了计算这些导数，我们调用 <code>loss.backward()</code> ，然后从 <code>w.grad</code> 和 <code>b.grad</code> 中检索值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">loss.backward()<br><span class="hljs-built_in">print</span>(w.grad)<br><span class="hljs-built_in">print</span>(b.grad)<br></code></pre></td></tr></table></figure>

<p>简而言之，PyTorch实现自动计算梯度的主要步骤如下：</p>
<ol>
<li>定义张量，并在需要计算梯度的张量上设置<code>requires_grad=True</code>。</li>
<li>执行计算操作，PyTorch会跟踪这些操作并构建计算图。</li>
<li>当需要计算梯度时，调用<code>.backward()</code>方法。PyTorch会根据计算图自动计算梯度，并将梯度累积到相应的张量的<code>.grad</code>属性中。</li>
</ol>
<p><strong>tips：</strong></p>
<p>我们只能获取计算图的叶节点的属性，这些节点的 <code>grad</code> <code>requires_grad</code> 属性设置为 <code>True</code> 。对于图中的所有其他节点，梯度将不可用。</p>
<h2 id="5-优化模型参数"><a href="#5-优化模型参数" class="headerlink" title="5.优化模型参数"></a>5.优化模型参数</h2><p>现在我们有了模型和数据，是时候通过优化模型的参数来训练、验证和测试我们的模型了。训练模型是一个迭代过程;在每次迭代中，模型对输出进行猜测，计算其猜测中的误差（损失），收集误差相对于其参数的导数（如我们在上一节中看到的），<strong>并使用梯度下降优化这些参数</strong>。</p>
<h3 id="5-1-超参数"><a href="#5-1-超参数" class="headerlink" title="5.1 超参数"></a>5.1 超参数</h3><p>超参数是可调整的参数，可用于控制模型优化过程。不同的超参数值会影响模型训练和收敛率</p>
<p>我们定义以下用于训练的超参数：</p>
<ul>
<li><strong>Number of Epochs</strong>  - <strong>遍历数据集的次数</strong></li>
<li><strong>Batch Size</strong> 批量大小 - 在<strong>更新参数之前</strong>通过网络传播的<strong>数据样本数</strong></li>
<li><strong>Learning Rate</strong> 学习率 - 在每个批次&#x2F;周期<strong>更新模型参数的程度</strong>。较小的值会导致学习速度较慢，而较大的值可能会导致训练期间出现不可预测的行为。</li>
</ul>
<h3 id="5-2-优化循环"><a href="#5-2-优化循环" class="headerlink" title="5.2 优化循环"></a>5.2 优化循环</h3><p>一旦我们设置了超参数，我们就可以使用优化循环来训练和优化我们的模型。优化循环的每次迭代称为一个纪元。</p>
<p>每个纪元由两个主要部分组成：</p>
<ul>
<li><strong>The Train Loop</strong> 训练循环 - 遍历训练数据集并尝试收敛到最佳参数。</li>
<li>**The Validation&#x2F;Test Loop **验证&#x2F;测试循环 - 遍历测试数据集，以检查模型性能是否正在提高。</li>
</ul>
<h3 id="5-3-损失函数"><a href="#5-3-损失函数" class="headerlink" title="5.3 损失函数"></a>5.3 损失函数</h3><p>当呈现一些训练数据时，我们未经训练的网络可能不会给出正确的答案。损失函数衡量获得的结果与目标值的差异程度，它是我们在训练过程中想要最小化的损失函数。为了计算损失，我们使用给定数据样本的输入进行预测，并将其与真实数据标签值进行比较。</p>
<h3 id="5-4-优化"><a href="#5-4-优化" class="headerlink" title="5.4 优化"></a>5.4 优化</h3><p>优化是调整模型参数以减少每个训练步骤中的模型误差的过程。优化算法定义了此过程的执行方式。所有优化逻辑都封装在对象中 <code>optimizer</code> 。在这里，我们使用 SGD 优化器;此外，PyTorch 中还有许多不同的优化器，例如 ADAM 和 RMSProp，它们更适合不同类型的模型和数据。</p>
<p>我们通过注册需要训练的模型参数并传入学习率超参数来初始化优化器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)<br></code></pre></td></tr></table></figure>

<p>在训练循环中，优化分三个步骤进行：</p>
<ul>
<li>调用 <code>optimizer.zero_grad()</code> 以重置模型参数的梯度。默认情况下，渐变相加;为了防止重复计算，我们在每次迭代时都明确地将它们归零。</li>
<li>通过调用  <code>loss.backward()</code> 反向传播预测损失。PyTorch 将损失的梯度与每个参数交汇。</li>
<li>一旦我们有了梯度，我们就会调用 <code>optimizer.step()</code> 通过向后传递中收集的梯度来调整参数。</li>
</ul>
<p>举例：</p>
<p><img src="/2024/01/24/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/Pytorch/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/pytorch%E5%9F%BA%E7%A1%80/7.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="5-5-全面实施"><a href="#5-5-全面实施" class="headerlink" title="5.5   全面实施"></a>5.5   全面实施</h3><p>我们在优化代码上定义 <code>train_loop</code> 循环，并 <code>test_loop</code> 根据我们的测试数据评估模型的性能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_loop</span>(<span class="hljs-params">dataloader, model, loss_fn, optimizer</span>):<br>    size = <span class="hljs-built_in">len</span>(dataloader.dataset)<br>    <span class="hljs-comment"># Set the model to training mode - important for batch normalization and dropout layers</span><br>    <span class="hljs-comment"># Unnecessary in this situation but added for best practices</span><br>    model.train()<br>    <span class="hljs-keyword">for</span> batch, (X, y) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(dataloader):<br>        <span class="hljs-comment"># Compute prediction and loss</span><br>        pred = model(X)<br>        loss = loss_fn(pred, y)<br><br>        <span class="hljs-comment"># Backpropagation</span><br>        loss.backward()<br>        optimizer.step()<br>        optimizer.zero_grad()<br><br>        <span class="hljs-keyword">if</span> batch % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>            loss, current = loss.item(), batch * batch_size + <span class="hljs-built_in">len</span>(X)<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;loss: <span class="hljs-subst">&#123;loss:&gt;7f&#125;</span>  [<span class="hljs-subst">&#123;current:&gt;5d&#125;</span>/<span class="hljs-subst">&#123;size:&gt;5d&#125;</span>]&quot;</span>)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">test_loop</span>(<span class="hljs-params">dataloader, model, loss_fn</span>):<br>    <span class="hljs-comment"># Set the model to evaluation mode - important for batch normalization and dropout layers</span><br>    <span class="hljs-comment"># Unnecessary in this situation but added for best practices</span><br>    model.<span class="hljs-built_in">eval</span>()<br>    size = <span class="hljs-built_in">len</span>(dataloader.dataset)<br>    num_batches = <span class="hljs-built_in">len</span>(dataloader)<br>    test_loss, correct = <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br><br>    <span class="hljs-comment"># Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode</span><br>    <span class="hljs-comment"># also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> dataloader:<br>            pred = model(X)<br>            test_loss += loss_fn(pred, y).item()<br>            correct += (pred.argmax(<span class="hljs-number">1</span>) == y).<span class="hljs-built_in">type</span>(torch.<span class="hljs-built_in">float</span>).<span class="hljs-built_in">sum</span>().item()<br><br>    test_loss /= num_batches<br>    correct /= size<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Test Error: \n Accuracy: <span class="hljs-subst">&#123;(<span class="hljs-number">100</span>*correct):&gt;<span class="hljs-number">0.1</span>f&#125;</span>%, Avg loss: <span class="hljs-subst">&#123;test_loss:&gt;8f&#125;</span> \n&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>我们初始化损失函数和优化器，并将其传递给 <code>train_loop</code> 和 <code>test_loop</code> 。随意增加 epoch 的数量以跟踪模型的改进性能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">loss_fn = nn.CrossEntropyLoss()<br>optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)<br><br>epochs = <span class="hljs-number">10</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Epoch <span class="hljs-subst">&#123;t+<span class="hljs-number">1</span>&#125;</span>\n-------------------------------&quot;</span>)<br>    train_loop(train_dataloader, model, loss_fn, optimizer)<br>    test_loop(test_dataloader, model, loss_fn)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Done!&quot;</span>)<br></code></pre></td></tr></table></figure>

<p><strong>tips：</strong></p>
<p><code>model.train()</code> 和 <code>model.eval()</code> 是 PyTorch 中用于控制模型模式的两个方法，它们的主要区别在于模型处于不同的运行模式，具体如下：</p>
<ol>
<li><code>model.train()</code>: 调用 <code>model.train()</code> 将模型设置为训练模式。在训练模式下，模型中的一些特定层，比如 dropout 和 batch normalization，会以不同的方式处理输入数据。例如，dropout 在训练时会随机丢弃部分节点，以防止过拟合；而 batch normalization 在训练时会根据当前 mini-batch 的统计数据来标准化输入数据。因此，在训练模式下，这些层会执行相应的训练操作。</li>
<li><code>model.eval()</code>: 调用 <code>model.eval()</code> 将模型设置为评估模式。在评估模式下，模型的行为会发生变化。例如，dropout 层不再随机丢弃节点，而是将所有节点保留，以便获取更加稳定的预测结果；batch normalization 也会使用固定的统计数据进行标准化，而不是使用当前 mini-batch 的统计数据。评估模式下，模型的行为更接近于实际使用场景。</li>
</ol>
<p>总的来说，<code>model.train()</code> 将模型设置为训练模式，用于训练过程中；<code>model.eval()</code> 将模型设置为评估模式，用于测试、验证或推断过程中，以获得更稳定和可靠的输出结果。</p>
<h3 id="5-6-保存并加载模型"><a href="#5-6-保存并加载模型" class="headerlink" title="5.6  保存并加载模型"></a>5.6  保存并加载模型</h3><p>在本节中，我们将了解如何通过保存、加载和运行模型预测来持久化模型状态。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision.models <span class="hljs-keyword">as</span> models<br></code></pre></td></tr></table></figure>

<p><strong>保存和加载模型权重</strong></p>
<p>PyTorch 模型将学习到的参数存储在名为 <code>state_dict</code> 的内部状态字典中。这些可以通过以下 <code>torch.save</code> 方法持久化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">model = models.vgg16(weights=<span class="hljs-string">&#x27;IMAGENET1K_V1&#x27;</span>)<br>torch.save(model.state_dict(), <span class="hljs-string">&#x27;model_weights.pth&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p>要加载模型权重，您需要先创建同一模型的实例，然后使用 <code>load_state_dict()</code> method 加载参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">model = models.vgg16() <span class="hljs-comment"># we do not specify ``weights``, i.e. create untrained model</span><br>model.load_state_dict(torch.load(<span class="hljs-string">&#x27;model_weights.pth&#x27;</span>))<br>model.<span class="hljs-built_in">eval</span>()<br></code></pre></td></tr></table></figure>

<p><strong>tips：</strong></p>
<p>请务必在推理前调用 <code>model.eval()</code> 方法，将 dropout 和 Batch 归一化层设置为评估模式。如果不这样做，将产生不一致的推理结果。</p>
<p><strong>Saving and Loading Models with Shapes</strong></p>
<p>在加载模型权重时，我们需要先实例化模型类，因为该类定义了网络的结构。我们可能希望将此类的结构与模型一起保存，在这种情况下，我们可以传递<code>model</code>（而不是 <code>model.state_dict()</code> ）到保存函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.save(model, <span class="hljs-string">&#x27;model.pth&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p>然后，我们可以像这样加载模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model = torch.load(<span class="hljs-string">&#x27;model.pth&#x27;</span>)<br></code></pre></td></tr></table></figure>


                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/AI-%CA%A2%E1%B5%95%E1%B4%97%E1%B5%95%CA%A1/" class="category-chain-item">AI ʢᵕᴗᵕʡ</a>
  
  
    <span>></span>
    
  <a href="/categories/AI-%CA%A2%E1%B5%95%E1%B4%97%E1%B5%95%CA%A1/Pytorch/" class="category-chain-item">Pytorch</a>
  
  
    <span>></span>
    
  <a href="/categories/AI-%CA%A2%E1%B5%95%E1%B4%97%E1%B5%95%CA%A1/Pytorch/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" class="category-chain-item">基础知识</a>
  
  

  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/AI/" class="print-no-link">#AI</a>
      
        <a href="/tags/Pytorch/" class="print-no-link">#Pytorch</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>pytorch基础!🍧</div>
      <div>https://yangchuanzhi20.github.io/2024/01/24/人工智能/Pytorch/基础知识/pytorch基础/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>白色很哇塞</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年1月24日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/01/30/%E7%AE%97%E6%B3%95/python/python%E6%96%B9%E6%B3%95%E5%92%8C%E6%8A%80%E5%B7%A7/pycharm%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7/" title="PyCharm使用技巧!🍩">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">PyCharm使用技巧!🍩</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/01/19/%E7%BD%91%E7%AB%99/%E7%BD%91%E7%AB%99%E5%AE%9E%E4%BE%8B/ieir2024/ieir2024%E7%BD%91%E7%AB%99%E6%90%AD%E5%BB%BA/" title="ieir2024网站搭建!🐧">
                        <span class="hidden-mobile">ieir2024网站搭建!🐧</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"uchVjxnRW51J2Mm7CW00jbKk-MdYXbMMI","appKey":"salCKdKKc0oJUGN300BBLXeE","path":"window.location.pathname","placeholder":"ʕ ᵔᴥᵔ ʔ","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"https://uchvjxnr.api.lncldglobal.com","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> <div style="font-size: 0.85rem"> <span id="timeDate">载入天数...</span> <span id="times">载入时分秒...</span> <script src="/vvd_js/duration.js"></script> </div> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>





  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  
<script src="/js/gongde.js"></script>
<script src="/js/move.js"></script>
<script src="/live2d-widget/autoload.js"></script>
<script src="/js/duration.js"></script>



<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<!-- hexo injector body_end start --><script src="/js/backgroundize.js"></script><!-- hexo injector body_end end --></body>
</html>
